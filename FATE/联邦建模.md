---
typora-root-url: img
---

[TOC]

# 联邦建模（github）

## 联邦学习

1. 联邦学习概述

什么是联邦学习呢？举例来说，假设有两个不同的企业 A 和 B，它们拥有不同数据。比 如，企业 A 有用户特征数据；企业 B 有产品特征数据和标注数据。这两个企业按照上述 GDPR 准则是不能粗暴地把双方数据加以合并的，因为数据的原始提供者，即他们各自的用户并没有机会来同意这样做。假设双方各自建立一个任务模型，每个任务可以是分类或预测，而这些任务也已经在获得数据时有各自用户的认可。那现在的问题是如何在 A 和 B 各端建立高质量的模型。但是，由于数据不完整（例如企业 A 缺少标签数据，企业 B 缺少特征数据）， 或者数据不充分 （数据量不足以建立好的模型），那么，在各端的模型有可能无法建立或效果并不理想。 联邦学习是要解决这个问题： 它希望做到各个企业的自有数据不出本地，而后联邦系统可以通过加密机制下的参数交换方式，即在不违反数据隐私法规情况下，建立一 个虚拟的共有模型。这个虚拟模型就好像大家把数据聚合在一起建立的最优模型一样。但是在建立虚拟模型的时候，数据本身不移动，也不泄露隐私和影响数据合规。这样，建好的模型在各自的区域仅为本地的目标服务。在这样一个联邦机制下，各个参与者的身份和地位相同，而联邦系统帮助大家建立了“共同富裕”的策略。 这就是为什么这个体系叫做“联邦 学习”。

2. 联邦学习的定义

当多个数据拥有方（例如企业）F_i, i=1…N 想要联合他们各自的数据 D_i 训练机器学习模型时，传统做法是把数据整合到一方并利用数据 D={Di，i=1…N}进行训练并得到模型 M_sum。然而，该方案由于其涉及到的隐私和数据安全等法律问题通常难以实施。为解决这一问题，我们提出联邦学习。联邦学习是指使得这些数据拥有方 F_i 在不用给出己方数据 D_i 的情况下也可进行模型训练并得到模型 M_FED 的计算过程，并能够保证模型 M_FED 的效果 V_FED 与模型 M_SUM 的效果 V_SUM 间的差距足够小,即： |V_FED-V_SUM |<δ， 这里 δ 是任意小的一个正量值。

## 联邦学习的分类

我们将以孤岛数据的分布特点为依据对联邦学习进行分类。 考虑有多个数据拥有方，每个数据拥有方各自所持有的数据集 D_i 可以用一个矩阵来表示。 矩阵的每一行代表一个用户，每一列代表一种用户特征。同时，某些数据集可能还包含标签数据。如果要对用户行为建立预测模型，就必须要有标签数据。我们可以把用户特征叫做 X， 把标签特征叫做 Y。比如，在金融领域，用户的信用是需要被预测的标签 Y；在营销领域， 标签是用户的购买愿望 Y；在教育领域，则是学生掌握知识的程度等。用户特征 X 加标签 Y 构成了完整的训练数据（X， Y）。但是，在现实中，往往会遇到这样的情况：各个数据集的用户不完全相同，或用户特征不完全相同。具体而言，以包含两个数据拥有方的联邦学习为例，数据分布可以分为以下三种情况：

两个数据集的用户特征（X1,X2,…）重叠部分较大，而用户(U1, U2…)重叠部分较小；
两个数据集的用户(U1, U2…)重叠部分较大，而用户特征（X1,X2,…）重叠部分较小；
两个数据集的用户(U1, U2…)与用户特征重叠（X1,X2,…）部分都比较小。为了应对以上三种数据分布情况，我们把联邦学习分为横向联邦学习、纵向联邦学习与联邦迁移学习。

- 横向联邦学习

在两个数据集的用户特征重叠较多而用户重叠较少的情况下，我们把数据集按照横向 (即**用户维度**)切分，并取出双方用户特征相同而用户不完全相同的那部分数据进行训练。这 种方法叫做横向联邦学习。比如有两家不同地区银行，它们的用户群体分别来自各自所在的地区，相互的交集很小。但是，它们的业务很相似，因此，记录的用户特征是相同的。此时， 就可以使用横向联邦学习来构建联合模型。Google 在 2017 年提出了一个针对安卓手机模型 更新的数据联合建模方案：在单个用户使用安卓手机时，不断在本地更新模型参数并将参数上传到安卓云上，从而**使特征维度相同的各数据拥有方建立联合模型的一种联邦学习方案**。

- 纵向联邦学习

在两个数据集的用户重叠较多而用户特征重叠较少的情况下，我们把数据集按照纵向 （即**特征维度**）切分，并取出双方用户相同而用户特征不完全相同的那部分数据进行训练。 这种方法叫做纵向联邦学习。比如有两个不同机构，一家是某地的银行，另一家是同一个地方的电商。它们的用户群体很有可能包含该地的大部分居民，因此用户的交集较大。但是，由于银行记录的都是用户的收支行为与信用评级，而电商则保有用户的浏览与购买历史，因此它们的用户特征交集较小。**纵向联邦学习就是将这些不同特征在加密的状态下加以聚合，以增强模型能力的联邦学习**。目前，逻辑回归模型，树型结构模型和神经网络模型等众多机器学习模型已经逐渐被证实能够建立在这个联邦体系上。

- 联邦迁移学习

在**两个数据集的用户与用户特征重叠都较少**的情况下，我们不对数据进行切分，而可以利用迁移学习来克服数据或标签不足的情况。这种方法叫做联邦迁移学习。 比如有两个不同机构，一家是位于中国的银行，另一家是位于美国的电商。由于受到地域限制，这两家机构的用户群体交集很小。同时，由于机构类型的不同，二者的数据特征也只有小部分重合。在这种情况下，要想进行有效的联邦学习，就必须引入迁移学习， 来解决单边数据规模小和标签样本少的问题，从而提升模型的效果。

## 联邦学习系统架构

我们以包含两个数据拥有方（即企业 A 和 B）的场景为例来介绍联邦学习的系统构架， 该构架可扩展至包含多个数据拥有方的场景。假设企业 A 和 B 想联合训练一个机器学习模 型，它们的业务系统分别拥有各自用户的相关数据。此外，企业 B 还拥有模型需要预测的标签数据。出于数据隐私和安全考虑，A 和 B 无法直接进行数据交换。此时，可使用联邦学习系统建立模型，系统构架由两部分构成，如下图所示。

第一部分：加密样本对齐。由于两家企业的用户群体并非完全重合，系统利用基于加密的用户样本对齐技术，在 A 和 B 不公开各自数据的前提下确认双方的共有用户，并且不暴露不互相重叠的用户。 以便联合这些用户的特征进行建模。
第二部分：加密模型训练。在确定共有用户群体后，就可以利用这些数据训练机器学习模型。为了保证训练过程中数据的保密性，需要借助第三方协作者 C 进行加密训练。以线 回归模型为例，训练过程可分为以下 4 步：

协作者 C 把公钥分发给 A 和 B，用以对训练过程中需要交换的数据进行加密；
A 和 B 之间以加密形式交互用于计算梯度的中间结果；
A 和 B 分别基于加密的梯度值进行计算，同时 B 根据其标签数据计算损失，并把这些结果汇总给 C，C 通过汇总结果计算总梯度并将其解密；
C 将解密后的梯度分别回传给 A 和 B；A 和 B 根据梯度更新各自模型的参数。
迭代上述步骤直至损失函数收敛，这样就完成了整个训练过程。在样本对齐及模型训练 过程中，A 和 B 各自的数据均保留在本地，且训练中的数据交互也不会导致数据隐私泄露。因此，双方在联邦学习的帮助下得以实现合作训练模型。
第三部分： 效果激励。联邦学习的一大特点就是它解决了为什么不同机构要加入联邦共同建模的问题， 即建立模型以后模型的效果会在实际应用中表现出来，并记录在永久数据记录机制（如区块链）上。 提供的数据多的机构会看到模型的效果也更好，这体现在对自己机构的贡献和对他人的贡献。这些模型对他人效果在联邦机制上以分给各个机构反馈，并继续激励更多机构加入这一数据联邦。
以上三个步骤的实施，即考虑了在多个机构间共同建模的隐私保护和效果，有考虑了如 何奖励贡献数据多的机构，以一个共识机制来实现。 所以， 联邦学习是一个“闭环”的学 习机制。

![](/联邦学习架构.png)

- 加密样本对齐

由于两家企业的用户群体并非完全重合，系统利用基于加密的用户样本对齐技术（这个方式有点像google的psi），在A和B不公开各自数据的前提下确认双方的共有用户，并且不暴露不互相重叠的用户。以便联合这些用户的特征进行建模。

- 加密模型训练

在确定共有用户群体后，就可以利用这些数据训练机器学习模型。为了保证训练过程中数据的保密性，需要借助第三方协作者C进行加密训练。以线性回归模型为例，训练过程可分为以下4步：

1. 协作者C把公钥分发给A和B，用以对训练过程中需要交换的数据进行加密；
2. A和B之间以加密形式交互用于计算梯度的中间结果；
3. A和B分别基于加密的梯度值进行计算，同时B根据其标签数据计算损失，并把这些结果汇总给C。C通过汇总结果计算总梯度并将其解密。
4. C将解密后的梯度分别回传给A和B；A和B根据梯度更新各自模型的参数。

迭代上述步骤直至损失函数收敛，这样就完成了整个训练过程。在样本对齐及模型训练过程种，A和B各自的数据均保留在本地，且训练中的数据交互也不会导致数据隐私泄露。因此，双方在联邦学习的帮助下得以实现合作训练模型。

## 联邦迁移学习

联邦迁移学习（Federated Transfer Learning），具体地，可以扩展已有的机器学习方法，使之具有FTL的能力。比如，我们可以将不同企业/不同来源的数据首先训练各自的模型，然后，将模型数据进行加密，使之不能直接传输以免泄露用户隐私。然后，在这个基础上，我们对这些模型进行联合训练，最后得出最优的模型，再返回给各个企业。

联邦迁移学习 vs 迁移学习 vs 多任务学习

从字面意思上看，FTL和迁移学习和多任务学习具有很强的相关性。它们的区别是：

多任务学习和FTL都注重多个任务的协同学习，最终目标都是要把所有的模型变得更强。但是，多任务学习强调不同任务之间可以共享训练数据，破坏了隐私规则；而FTL则可以在不共享隐私数据的情况下，进行协同的训练。

迁移学习注重知识从一个源领域到另一个目标领域的单向迁移。而这种单向的知识迁移，往往伴有一定的信息损失：因为我们通常只会关注迁移学习在目标领域上的效果，而忽略了在源领域上的效果。FTL则从目标上就很好地考虑了这一点：多个任务之间协同。

当然，迁移学习和多任务学习都可以解决模型和数据漂移的问题，这一点在FTL中也得到了继承。

## DAG

### 什么是DAG

DAG（Database Availability Group）即数据库可用性组。

在spark里每一个操作生成一个RDD，RDD之间连一条边，最后这些RDD和他们之间的边组成一个有向无环图，这个就是DAG。

### 为什么要创建DAG？

- 用来解决数据容错的高效性
- 其二用来划分stage

RDD的依赖关系分为两种：宅以来（Narrow Dependencies）与宽依赖（Wide Dependencies，源码中称为Shuffle Dependencies）

![](/DAG依赖类型.png)

- 窄依赖

每个父RDD的一个Partition最多被子RDD的一个Partition所使用（1：1或n：1）。例如map、filter、union等操作都会产生窄依赖：
`子RDD分区通常对应常数个父RDD分区(O(1)，与数据规模无关）。`

- 宽依赖

一个父RDD的Partition会被多个子RDD的Partition所使用，例如：groupByKey、reduceByKey、sortByKey等操作都会产生宽依赖；（1：m或n：m）

`子RDD分区通常对应所有的父RDD分区（O(n)，与数据规模有关）`

### DAG的生成

![DAG的生成](/DAG的生成.jpg)

原始的RDD通过一系列的转换就形成了DAG，有了计算的DAG图，Spark内核下一步的任务就是根据DAG图将计算划分成任务集，也就是stage，这样可以将任务提交到计算节点进行真正的计算。Spark计算的中间结果是保存在内存中的，Spark在划分stage的时候会充分考虑在分布式计算中可流水线计算（pipeline）的部分来提高计算的效率 ，而在这个过程中spark根据RDD之间依赖关系的不同将DAG划分成不同的stage（调度阶段）。对于`窄依赖`，partition的转换处理在一个stage中完成计算。对于`宽依赖`，由于有shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此`宽依赖是划分stage的依据`。

- DAG工作原理

![DAG工作原理](/DAG工作原理.png)

Spark执行时有下面所列的流程：

1. 用户代码定义RDD的有向-无环-图

RDD上的操作会创建新的RDD，并引用他们的父节点，这样就创建了一个图。

2. 行动操作把有向-无环-图强制转译为执行计划

当调用RDD的一个行动操作时，这个RDD就必须被计算出来。这也要求计算出该RDD的父节点。Spark调度器提交一个作业来计算出所有必要的RDD。这个作业会包含一个或多个步骤，每个步骤其实也就是一波并行执行的计算任务。一个步骤对应有向-无环-图中的一个或多个RDD，一个步骤对应多个RDD是因为发生了流水线执行。

3. 任务于集群中调度并执行

步骤是按顺序处理的，任务则独立的启动来计算出RDD的一部分。一旦作业的最后一个步骤结束，